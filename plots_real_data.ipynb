{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook heavily uses the code from https://github.com/felipemaiapolo/cit.git to reproduce their results.\n",
    "It assumes the simulated null experiment (`run_h0_car_data.py`) results are saved to `./car-final.pt` and that the p-value experiment results are provided here (`run_pval_car_data.py` simply prints those results).\n",
    "\n",
    "It's better to run this notebook in colab due to extra dependencies + the need to run RBPT locally. Overall, running the notebook should take well under an hour to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/felipemaiapolo/cit.git\n",
    "%cd ./cit\n",
    "# !pip install -r requirements.txt\n",
    "!pip install catboost  # the only one missing from colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general import *\n",
    "import time\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import seaborn as sns\n",
    "\n",
    "cpu=mp.cpu_count()\n",
    "cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import copy\n",
    "import time\n",
    "from general import *\n",
    "from exp1 import get_pval_stfr, get_pval_crt\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "\n",
    "###Regression\n",
    "def get_pval_gcm(X, Z, Y, g2, p_model):\n",
    "    n = X.shape[0]\n",
    "    rx = X-p_model.predict_proba(Z)[:,1].reshape(X.shape)\n",
    "    ry = Y-g2.predict(None, Z)\n",
    "    T = rx.squeeze()*ry.squeeze()\n",
    "    pval = 2*(1 - scipy.stats.norm.cdf(abs(np.sqrt(n)*np.mean(T)/np.std(T))))\n",
    "    return pval\n",
    "\n",
    "def get_pval_rbpt(X, Z, Y, H, g1, loss='mse'):\n",
    "    n = X.shape[0]\n",
    "    XZ = np.hstack((X, Z))\n",
    "    loss1 = get_loss(Y, g1.predict(X,Z).reshape((-1,1)), loss=loss)\n",
    "    loss2 = get_loss(Y, H.reshape((-1,1)), loss=loss)\n",
    "    T = loss2-loss1\n",
    "    pval = 1 - scipy.stats.norm.cdf(np.sqrt(n)*np.mean(T)/np.std(T))\n",
    "    return pval\n",
    "\n",
    "def get_pval_rbpt2(X, Z, Y, g1, h, loss='mae'):\n",
    "    n = X.shape[0]\n",
    "    XZ = np.hstack((X, Z))\n",
    "    loss1 = get_loss(Y, g1.predict(X,Z).reshape((-1,1)), loss=loss)\n",
    "    loss2 = get_loss(Y, h.predict(Z).reshape((-1,1)), loss=loss)\n",
    "    T = loss2-loss1\n",
    "    pval = 1 - scipy.stats.norm.cdf(np.sqrt(n)*np.mean(T)/np.std(T))\n",
    "    return pval\n",
    "\n",
    "\n",
    "# Our addition:\n",
    "def get_pval_rbpt2_ub(X, Z, Y, g1, h, loss='mse'):\n",
    "    assert loss == 'mse'\n",
    "    n = X.shape[0]\n",
    "    XZ = np.hstack((X, Z))\n",
    "    g_pred = g1.predict(X,Z).reshape((-1,1))\n",
    "    h_pred = h.predict(Z).reshape((-1,1))\n",
    "    loss1 = get_loss(Y, g_pred, loss=loss)\n",
    "    loss2 = get_loss(Y, h_pred, loss=loss)\n",
    "    bias_correction = get_loss(g_pred, h_pred, loss=loss)\n",
    "    #!!!!!!!!!\n",
    "    T = loss2-loss1 + bias_correction\n",
    "    pval = 1 - scipy.stats.norm.cdf(np.sqrt(n)*np.mean(T)/np.std(T))\n",
    "    return pval\n",
    "\n",
    "\n",
    "def get_h(X, y, validation_split=.1, verbose=False, random_state=None):\n",
    "\n",
    "    ### Paramaters\n",
    "    early_stopping_rounds=10\n",
    "    loss='MultiRMSE'\n",
    "\n",
    "    ### Validating\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, random_state=random_state)\n",
    "\n",
    "    m = CatBoostRegressor(loss_function = loss,\n",
    "                          eval_metric = loss,\n",
    "                          thread_count=-1,\n",
    "                          random_seed=random_state)\n",
    "\n",
    "    m.fit(X_train, y_train, verbose=verbose,\n",
    "          eval_set=(X_val, y_val),\n",
    "          early_stopping_rounds = early_stopping_rounds)\n",
    "\n",
    "\n",
    "    ### Final model\n",
    "    m2 = CatBoostRegressor(iterations=int(m.tree_count_),\n",
    "                           loss_function = loss,\n",
    "                           eval_metric = loss,\n",
    "                           thread_count=-1,\n",
    "                           random_seed=random_state)\n",
    "\n",
    "    m2.fit(X, y, verbose=verbose)\n",
    "\n",
    "    return m2\n",
    "\n",
    "\n",
    "#####\n",
    "def exp2(it, n_vals, loss, alpha, B):\n",
    "\n",
    "    states=['ca','il','mo','tx']\n",
    "    pvals=[]\n",
    "    times=[]\n",
    "    count=0\n",
    "\n",
    "    for s in states:\n",
    "\n",
    "        data = pd.read_csv('data/car-insurance-public/data/' + s + '-per-zip.csv')\n",
    "        companies = list(set(data.companies_name))\n",
    "\n",
    "        for cia in companies:\n",
    "\n",
    "            data = pd.read_csv('data/car-insurance-public/data/' + s + '-per-zip.csv')\n",
    "            data = data.loc[:,['state_risk','combined_premium','minority','companies_name']].dropna()\n",
    "            data = data.loc[data.companies_name == cia]\n",
    "\n",
    "            Z = np.array(data.state_risk).reshape((-1,1))\n",
    "            # print(Z.shape)\n",
    "            # continue\n",
    "            Y = np.array(data.combined_premium).reshape((-1,1))\n",
    "            X = (1*np.array(data.minority)).reshape((-1,1))\n",
    "\n",
    "            #bins = np.percentile(Z, np.linspace(0,100,n_vals+2))\n",
    "            bins = np.linspace(np.min(Z),np.max(Z),n_vals+2)\n",
    "            bins = bins[1:-1]\n",
    "            Y_ci = copy.deepcopy(Y)\n",
    "            Z_bin = np.array([find_nearest(bins, z) for z in Z.squeeze()]).reshape(Z.shape)\n",
    "\n",
    "            for val in np.unique(Z_bin):\n",
    "                ind = Z_bin==val\n",
    "                rng = np.random.RandomState(it)\n",
    "                ind2 = rng.choice(np.sum(ind),np.sum(ind),replace=False)\n",
    "                Y_ci[ind] = Y_ci[ind][ind2]\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test, Z_train, Z_test = train_test_split(X, Y_ci, Z_bin, test_size=.3, random_state=it)\n",
    "\n",
    "            ###Fitting models\n",
    "            g1 = g()\n",
    "            g1.fit(X_train, Z_train, Y_train)\n",
    "            g2 = g()\n",
    "            g2.fit(None, Z_train, Y_train)\n",
    "\n",
    "            ###RBPT\n",
    "            start_time = time.time()\n",
    "            p = LogisticRegressionCV(cv=5, scoring='neg_log_loss', solver='liblinear', random_state=0).fit(Z_train, X_train.squeeze())\n",
    "            H_test = np.sum(p.predict_proba(Z_test)*np.hstack((g1.predict(np.zeros(X_test.shape),Z_test).reshape(-1,1),\n",
    "                                                               g1.predict(np.ones(X_test.shape),Z_test).reshape(-1,1))), axis=1).reshape(-1,1)\n",
    "            pval_rbpt = get_pval_rbpt(X_test, Z_test, Y_test, H_test, g1, loss=loss)\n",
    "            time_rbpt = time.time() - start_time\n",
    "\n",
    "            ###RBPT2\n",
    "            start_time = time.time()\n",
    "            h = get_h(Z_train, g1.predict(X_train,Z_train).squeeze())\n",
    "            pval_rbpt2 = get_pval_rbpt2(X_test, Z_test, Y_test, g1, h, loss=loss)\n",
    "            time_rbpt2 = time.time() - start_time\n",
    "\n",
    "\n",
    "            ###RBPT2 unbiased\n",
    "            start_time = time.time()\n",
    "            pval_rbpt2_ub = get_pval_rbpt2_ub(X_test, Z_test, Y_test, g1, h, loss=loss)\n",
    "            time_rbpt2_ub = time.time() - start_time\n",
    "\n",
    "\n",
    "            ###GCM\n",
    "            start_time = time.time()\n",
    "            pval_gcm = get_pval_gcm(X_test, Z_test, Y_test, g2, p)\n",
    "            time_gcm = time.time() - start_time\n",
    "\n",
    "\n",
    "            ###Storing\n",
    "            times.append([count, time_rbpt, time_rbpt2, time_rbpt2_ub, time_gcm])\n",
    "            pvals.append([count, pval_rbpt, pval_rbpt2, pval_rbpt2_ub, pval_gcm])\n",
    "\n",
    "        count+=1\n",
    "\n",
    "    pvals = np.array(pvals)\n",
    "    return pvals, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['California','Illinois','Missouri','Texas']\n",
    "alpha = 0.05\n",
    "loss = 'mse'\n",
    "B = 100\n",
    "n_vals = 20\n",
    "iterations = 50 #48\n",
    "\n",
    "pool = mp.Pool(cpu)\n",
    "out = pool.starmap(exp2, [(it, n_vals, loss, alpha, B) for it in range(iterations)])\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBPT paper results done, now loading the kernel-based measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load('car-final.pt')\n",
    "res_kernels = np.zeros((4, 50, 3))\n",
    "\n",
    "for i in range(50):\n",
    "    for st_idx, state in enumerate(['ca', 'il', 'mo', 'tx']):\n",
    "        for al_idx, alg in enumerate(['kci', 'kci_xsplit', 'circe']):\n",
    "            res_kernels[st_idx, i, al_idx] = (np.array(res[i][state][alg])[:, 1] < alpha).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.shape = (50, 98, 5), so 50 runs, pvals/times, 98 runs with grouping, last dim [0] is count of state\n",
    "rejection_rate = np.zeros((4, out.shape[0], out.shape[-1] - 1))\n",
    "alpha = 0.05\n",
    "# essentially we define the acceptance rate as the average acceptance rate across companies, and estimate that.\n",
    "\n",
    "for state_idx in range(4):\n",
    "    state_vals = out[:, out[0, :, 0] == state_idx, 1:] < alpha\n",
    "    rejection_rate[state_idx] = state_vals.mean(axis=1)\n",
    "    \n",
    "rejection_rate = np.concatenate((rejection_rate, res_kernels[:, :, :]), axis=-1) # with circe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_rate_for_pdf = np.zeros((3, np.prod(rejection_rate.shape)))\n",
    "labels = np.zeros_like(rejection_rate)\n",
    "for state_idx in range(4):\n",
    "    for algorithm in range(rejection_rate.shape[-1]):\n",
    "        l = rejection_rate.shape[1] * (algorithm + state_idx * rejection_rate.shape[-1])\n",
    "        rejection_rate_for_pdf[0, l:l+rejection_rate.shape[1]] = state_idx\n",
    "        rejection_rate_for_pdf[1, l:l+rejection_rate.shape[1]] = algorithm\n",
    "        rejection_rate_for_pdf[2, l:l+rejection_rate.shape[1]] = rejection_rate[state_idx, :, algorithm]\n",
    "        \n",
    "df = pd.DataFrame(rejection_rate_for_pdf.T)\n",
    "\n",
    "for idx, state_name in enumerate(['California','Illinois','Missouri','Texas']):\n",
    "    df.loc[df[0] == idx, 0] = state_name\n",
    "\n",
    "for idx, alg_name in enumerate(['RBPT','RBPT2','RBPT2\\'','GCM', 'KCI', 'SplitKCI', 'CIRCE']): #\n",
    "    df.loc[df[1] == idx, 1] = alg_name\n",
    "\n",
    "df.rename(columns={0: 'State', 1: 'Algorithm', 2: 'Type I error'}, inplace=True)\n",
    "\n",
    "grid = sns.catplot(\n",
    "    df, kind=\"bar\",\n",
    "    x='Algorithm', y='Type I error', col='State',\n",
    "    height=7, aspect=.6,\n",
    "    errorbar='se', alpha=0.5\n",
    ")\n",
    "\n",
    "axes = grid.axes.flatten()\n",
    "\n",
    "# iterate through the axes\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axhline(alpha, ls='--', c='#5F5F5F')\n",
    "\n",
    "plt.savefig('./figs/data_h0_catplot_with_circe.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states=['ca','il','mo','tx']\n",
    "labels = ['California','Illinois','Missouri','Texas']\n",
    "alpha = 0.05\n",
    "loss = 'mse'\n",
    "B = 100\n",
    "\n",
    "data = pd.read_csv('data/car-insurance-public/data/mo-per-zip.csv')\n",
    "data.head()\n",
    "\n",
    "random_state=42\n",
    "import torch\n",
    "# np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []\n",
    "times = []\n",
    "\n",
    "for s in tqdm(states):\n",
    "    data = pd.read_csv('data/car-insurance-public/data/' + s + '-per-zip.csv')\n",
    "\n",
    "    Z = np.array(data.state_risk).reshape((-1,1))\n",
    "    Y = np.array(data.combined_premium).reshape((-1,1))\n",
    "    X = (1*np.array(data.minority)).reshape((-1,1))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, random_state=random_state)\n",
    "    Z_train, Z_test, _, _ = train_test_split(Z, Y, test_size=.3, random_state=random_state)\n",
    "\n",
    "    ###Fitting models\n",
    "    g1 = g()\n",
    "    g1.fit(X_train, Z_train, Y_train)\n",
    "    g2 = g()\n",
    "    g2.fit(None, Z_train, Y_train)\n",
    "\n",
    "    ###RBPT\n",
    "    p = LogisticRegressionCV(cv=5, scoring='neg_log_loss', solver='liblinear', random_state=0).fit(Z_train, X_train.squeeze())\n",
    "    H_test = np.sum(p.predict_proba(Z_test)*np.hstack((g1.predict(np.zeros(X_test.shape),Z_test).reshape(-1,1),\n",
    "                                                       g1.predict(np.ones(X_test.shape),Z_test).reshape(-1,1))), axis=1).reshape(-1,1)\n",
    "    pval_rbpt = get_pval_rbpt(X_test, Z_test, Y_test, H_test, g1, loss=loss)\n",
    "\n",
    "    ###RBPT2\n",
    "    h = get_h(Z_train, g1.predict(X_train,Z_train).squeeze())\n",
    "    pval_rbpt2 = get_pval_rbpt2(X_test, Z_test, Y_test, g1, h, loss=loss)\n",
    "\n",
    "    ###RBPT2 unbiased\n",
    "    start_time = time.time()\n",
    "    pval_rbpt2_ub = get_pval_rbpt2_ub(X_test, Z_test, Y_test, g1, h, loss=loss)\n",
    "    time_rbpt2_ub = time.time() - start_time\n",
    "    \n",
    "    ###GCM\n",
    "    pval_gcm = get_pval_gcm(X_test, Z_test, Y_test, g2, p)\n",
    "\n",
    "\n",
    "    ###Storing\n",
    "    # pvals.append([pval_rbpt, pval_rbpt2, pval_stfr, pval_gcm, pval_crt, pval_cpt])\n",
    "    pvals.append([pval_rbpt, pval_rbpt2, pval_rbpt2_ub, pval_gcm])\n",
    "\n",
    "pvals=np.array(pvals)\n",
    "print(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from 'run_pval_car_data.py' output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# circe:\n",
    "circe_res = np.array([0.003, 0., 0.003, 0.002])\n",
    "\n",
    "# kci\n",
    "kci_res = np.array([0.016, 0.,    0.,    0.,   ])\n",
    "\n",
    "# split-kci\n",
    "split_kci_res = np.array([0.027, 0.,    0.,    0.,   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_new = np.hstack((pvals, kci_res[:, None], split_kci_res[:, None], circe_res[:, None]))\n",
    "pvals_old = pvals.copy()\n",
    "pvals = pvals_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.zeros((3, np.prod(pvals.shape)))\n",
    "\n",
    "for state_idx in range(4):\n",
    "    for alg_idx in range(pvals.shape[1]):\n",
    "        res[0, alg_idx + state_idx * pvals.shape[1]] = state_idx\n",
    "        res[1, alg_idx + state_idx * pvals.shape[1]] = alg_idx\n",
    "        res[2, alg_idx + state_idx * pvals.shape[1]] = pvals[state_idx, alg_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res.T)\n",
    "\n",
    "for idx, state_name in enumerate(['California','Illinois','Missouri','Texas']):\n",
    "    df.loc[df[0] == idx, 0] = state_name\n",
    "\n",
    "for idx, alg_name in enumerate(['RBPT','RBPT2','RBPT2\\'','GCM', 'KCI', 'SplitKCI', 'CIRCE']):\n",
    "    df.loc[df[1] == idx, 1] = alg_name\n",
    "\n",
    "df.rename(columns={0: 'State', 1: 'Algorithm', 2: 'p-value'}, inplace=True)\n",
    "\n",
    "grid = sns.catplot(\n",
    "    df, kind=\"bar\",\n",
    "    x='Algorithm', y='p-value', col='State',\n",
    "    height=7, aspect=.6,\n",
    "    errorbar='se', alpha=0.5\n",
    ")\n",
    "\n",
    "axes = grid.axes.flatten()\n",
    "\n",
    "# iterate through the axes\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axhline(alpha, ls='--', c='#5F5F5F')\n",
    "\n",
    "plt.savefig('./figs/data_pval_catplot_circe.pdf')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv_eg_pt2",
   "language": "python",
   "name": "ffcv_eg_pt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
